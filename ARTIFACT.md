# MIRAGE-OG++ Artifact Guide

This repo is structured as a **paper artifact**: a runnable end-to-end demo + tests + a micro-benchmark.

## What You Can Reproduce (No External Keys)

1. Correctness + privacy plumbing (DPF/FSS-PIR, handles, MCP)
2. End-to-end allow/deny behavior for benign + malicious flows
3. A micro-benchmark showing **DPF key size is O(log N)** vs the old O(N) one-hot baseline
4. Capsule mediation smoke (macOS sandbox-exec; Linux bwrap if available)
5. SkillDocBench v1 (100 skill docs) + ingress decisions
6. Transcript-classifier leakage eval (unshaped vs shaped) from a single policy server view
7. Local vs outsourced rules comparison (why PIR/MPC is needed for centralized DBs without single-auditor query leakage)
8. Baselines / ablations (no-capsule exfil, insecure executor, loopback-HTTP capsule)

### Run (Core Artifact)

```bash
pip install -r requirements.txt
python main.py artifact
# or:
bash scripts/run_artifact.sh
```

Outputs are written to `artifact_out/`:

- `artifact_out/unittest.txt`
- `artifact_out/bench_fss.txt`
- `artifact_out/report.json` (machine-readable end-to-end result)
  - includes: `capsule_smoke`, `skilldocbench`, `leakage_eval`, `executor_bypass_attempts`, etc.
- `artifact_out/bench_e2e.json` (short end-to-end throughput numbers; Python policy server)
- `artifact_out/bench_e2e.rust.json` (same throughput workload; Rust policy server backend, if `cargo` is available)
- `artifact_out/audit.jsonl` (gateway audit trail; JSONL)

## Paper-Grade Pipeline (Formal + Baselines + Plots)

The core artifact above is intentionally self-contained. For a paper-grade run
that adds formal checks, strong baselines, larger evals, throughput curves, and
auto-generated figures, run:

```bash
python main.py paper-artifact
# or:
bash scripts/run_paper_artifact.sh
```

Additional outputs (all under `artifact_out/`):

- `artifact_out/security_game_nbe.json` (formal NBE game / theorem harness output)
- `artifact_out/paper_eval/paper_eval_rows.csv`
- `artifact_out/paper_eval/paper_eval_summary.json`
- `artifact_out/policy_perf/policy_server_curves.json`
- `artifact_out/policy_perf/policy_server_curves.csv`
- `artifact_out/native_baselines/native_guardrail_eval.json` (Codex/Claude/OpenClaw native baselines)
- `artifact_out/campaign/real_agent_campaign.json` (real-agent closed-loop evidence chain)
- `artifact_out/figures/*.svg` + `artifact_out/figures/figures_index.json`
- `artifact_out/repro_manifest.json` (versions + seed + platform manifest)

Notes:

- Some steps make external model calls (native baselines, OpenClaw/NanoClaw runs). The pipeline
  is written to `SKIP/continue` when credentials are missing so the overall run remains usable.
- Paper-grade settings can be overridden via env vars; see `scripts/run_paper_artifact.sh`.
- Formal definitions live in `FORMAL_SECURITY.md`.

## “Real Agent” Integration (NanoClaw Runtime)

NanoClaw uses the **Claude Agent SDK** runtime. This repo includes a small runner that exercises `mirage.act` via **MCP** from that runtime.

### Requirements

- Node.js 20+ (your machine already has Node if `node -v` works)
- One of:
  - `ANTHROPIC_API_KEY` (recommended), or
  - `CLAUDE_CODE_OAUTH_TOKEN`

### Run

```bash
# Optional: fetch upstream nanoclaw (pinned commit recorded in NANOCLAW_VERSION.txt)
bash scripts/setup_nanoclaw.sh

# Run the MCP-connected agent via Claude Agent SDK
export ANTHROPIC_API_KEY="..."
bash scripts/run_nanoclaw.sh
```

Notes:

- `scripts/run_nanoclaw.sh` starts: policy servers + executor, then runs
  `integrations/nanoclaw_runner/mirage_demo.mjs` (Claude Agent SDK runtime).
- This step makes external model calls; it is not required for the core artifact
  and is intentionally separated from `scripts/run_artifact.sh`.

## “Real Agent” Integration (OpenClaw + OpenAI OAuth)

If you want to validate the same protections using **OpenAI OAuth** (via OpenClaw's `openai-codex` provider), run:

```bash
bash scripts/setup_openclaw.sh

# Authenticate once with Codex CLI, then import the OAuth tokens into OpenClaw state.
# (scripts/run_openclaw.sh will also auto-import if auth is missing.)
OPENCLAW_STATE_DIR="artifact_out/openclaw_state" \
  python scripts/import_codex_oauth_to_openclaw.py

# Run benign + malicious scenarios; outputs are written to artifact_out/
bash scripts/run_openclaw.sh
```

Outputs:

- `artifact_out/openclaw_benign.json`
- `artifact_out/openclaw_malicious.json`

Notes:

- This step makes external model calls and is intentionally separated from `scripts/run_artifact.sh`.
- The OpenClaw config generated by `scripts/run_openclaw.sh` restricts core tools to a minimal set and exposes **only one side-effect tool**: `mirage_act` (a bridge to the Python MIRAGE MCP gateway).
- OpenClaw requires `node >= 22.12.0`.
- OpenClaw's interactive Codex OAuth flow (`openclaw models auth login --provider openai-codex`) requires a provider plugin. This repo ships one at `integrations/openclaw_runner/extensions/openai-codex-auth/` and a helper:
  - `OPENCLAW_STATE_DIR="artifact_out/openclaw_state" bash scripts/setup_openclaw_state.sh`
  For artifact determinism we still recommend importing from `~/.codex/auth.json`.

## Optional: Rust Policy Server Backend

If you have a Rust toolchain available, you can switch policy servers to the compiled backend:

```bash
POLICY_BACKEND=rust bash scripts/run_artifact.sh
```

## Optional: Hide `db_name` (Policy Bundle)

To hide which logical DB is queried from each policy server, build a bundled DB and enable it in the gateway:

```bash
POLICY_BUNDLE_ENABLE=1 USE_POLICY_BUNDLE=1 bash scripts/run_artifact.sh
```

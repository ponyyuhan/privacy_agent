# Performance (Production-Oriented Narrative, Artifact-Faithful)

This document summarizes the artifact's performance evidence with an emphasis on:

1. The **policy server** as the dominant $O(N)$ kernel (bitset inner-product / PRG expansion).
2. The impact of **constant-shape traffic** (mixing + padding + cover traffic) on end-to-end throughput and latency.
3. Scaling with **threads** and the effect of **binary vs JSON** transport.

All numbers referenced here are generated by scripts in this repo and written as JSON/CSV artifacts.

---

## 1. What We Measure

### 1.1 Policy Server Kernel Throughput

We benchmark the policy server endpoints used by PIR/FSS (membership) and MPC (commit proof shares).

Outputs:

- `artifact_out_perf_v3/policy_perf/policy_server_curves.json`
  - sweep over batch size and `pad_to` (effective vs padded work).
- `artifact_out_perf_v3/policy_perf/policy_server_scaling.json`
  - sweep over thread count and wire format (`json` vs `bin`).

Reproduce:

```bash
OUT_DIR=artifact_out_perf_v3 python scripts/bench_policy_server_curves.py
OUT_DIR=artifact_out_perf_v3 python scripts/bench_policy_server_scaling.py
```

### 1.2 End-to-End Constant-Shape Throughput and Latency

We benchmark the complete request path (gateway + 2 policy servers + executor) under:

- baseline (no mixing / no cover), and
- constant-shape schedule (mixing + padding + cover traffic), parameterized by `pad_to`.

Outputs:

- `artifact_out_perf_v3/shaping_perf/e2e_shaping_curves.json`

Reproduce:

```bash
OUT_DIR=artifact_out_perf_v3 python scripts/bench_e2e_shaping_curves.py
```

---

## 2. Fixed-Shape (Pad + Cover + Mixer) Curves: What They Demonstrate

The constant-shape design enforces a schedule where the policy servers observe a fixed number of subrequests per tick, with dummy work filling any slack.

From a performance perspective, this introduces two types of overhead:

1. **Dummy compute**: padding increases total $O(N)$ inner products.
2. **Scheduler overhead**: mixing introduces queueing, tick granularity, and batching logic.

From a privacy perspective, it reduces load-shape and intent/db selection leakage (see `LEAKAGE_MODEL.md`).

The artifact outputs both throughput and tail latency under these shaping knobs so the paper can state:

> Under an explicitly fixed leakage shape (pad + cover + mixer), throughput remains in a usable regime and does not degrade by orders of magnitude.

---

## 3. Multi-Core Scaling and Transport Choice

The policy server benchmarks include:

- `wire=json` (HTTP + JSON; baseline for readability)
- `wire=bin` (binary framing; reduced parse and copy overhead)

and scale the compute thread pool (Rust uses Rayon when enabled).

The scaling data is intended to support a paper claim of the form:

> The dominant cost is the $O(N)$ evaluation kernel; using a compiled backend and binary transport yields materially higher key-throughput per host, and multi-core parallelism is effective up to the point where non-kernel overheads dominate.

We intentionally report both:

- throughput (keys/s or effective-keys/s), and
- p50/p95 latency,

because constant-shape compilation often trades throughput for latency (and vice versa) depending on mixer tick size.

---

## 4. Bottleneck Attribution (Artifact-Level)

In this artifact, the primary bottlenecks are:

1. **Policy server $O(N)$ evaluation** at large domain sizes (inner-product kernel dominates).
2. **Per-request overhead** (serialization, network framing, Python orchestration) when batch size is small.
3. **Mixer tick and queueing** when constant-shape is enabled (pad + cover).

The benchmarking scripts isolate these effects by:

- sweeping batch size (amortization of fixed overhead),
- sweeping `pad_to` (dummy work factor), and
- toggling wire format and thread count.

---

## 5. What Still Needs to Be Done for “Production-Grade” Performance

This repo already includes a compiled Rust backend. For production-grade performance engineering, the remaining high-impact steps are:

1. **SIMD/vectorized bitset inner products** (Rust `std::arch` or C++ intrinsics) for the $O(N)$ kernel.
2. **True batch evaluation** for many DPF keys per request with minimal per-batch overhead.
3. **Long-lived binary connections** (UDS for same-host deployments, HTTP/2 or custom framing) to reduce per-request setup.
4. **Global mixer** across sessions (high utilization constant-shape scheduling) to avoid low-load inefficiency under cover traffic.

These are engineering steps; the leakage model must remain explicit (the paper should state the constant-shape contract).

